{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file uses a Python2 library from Github (https://github.com/vadimkantorov/caffemodel2pytorch) to convert Caffe models into PyTorch. The original model was trained with Caffe. Remarkably, you don't ever have to install caffe for it to work!\n",
    "\n",
    "Note this script is also in Python2.\n",
    "\n",
    "Run `merge_npys` to combine the files created in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caffemodel2pytorch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caffemodel2pytorch: loading model from [predicting_poverty_trained.caffemodel] in HDF5 format failed [Unable to open file (file signature not found)], falling back to caffemodel format\n",
      "caffemodel2pytorch: loaded model from [predicting_poverty_trained.caffemodel] in caffemodel format\n"
     ]
    }
   ],
   "source": [
    "model = Net(\n",
    "    prototxt = 'predicting_poverty_deploy.prototxt',\n",
    "    weights = 'predicting_poverty_trained.caffemodel',\n",
    "    caffe_proto = 'https://raw.githubusercontent.com/BVLC/caffe/master/src/caffe/proto/caffe.proto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Convolution(3, 64, kernel_size=(11, 11), stride=(4, 4))\n",
       "  (relu1): ReLU()\n",
       "  (norm1): LocalResponseNorm(5, alpha=0.000500000023749, beta=0.75, k=1.0)\n",
       "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Convolution(64, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (relu2): ReLU()\n",
       "  (norm2): LocalResponseNorm(5, alpha=0.000500000023749, beta=0.75, k=1.0)\n",
       "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (conv4): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4): ReLU()\n",
       "  (conv5): Convolution(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu5): ReLU()\n",
       "  (pool5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv6): Convolution(256, 4096, kernel_size=(6, 6), stride=(6, 6))\n",
       "  (relu6): ReLU()\n",
       "  (conv7): Convolution(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu7): ReLU()\n",
       "  (conv8): Convolution(4096, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (pool6): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       "  (prob): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we strip the last layers, so the output is just what was at the conv7 layer\n",
    "# the paper uses the data at this layer as the \"features\" for the image\n",
    "del model.prob, model.pool6, model.conv8, model.relu7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f3f1013f790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CPU for detection\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the data into a PyTorch Tensor\n",
    "data_transforms = {\n",
    "    'transform': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# takes a \n",
    "def filename_to_im_tensor(file):\n",
    "    im = plt.imread(file)[:,:,:3]\n",
    "    im = (im*256)\n",
    "    im -= np.array([103.334, 107.8797, 107.4072])\n",
    "    # convert to BGR, their Github code says to do this\n",
    "    input_img = im[:, :, [2, 1, 0]]\n",
    "    # convert to D,H,W\n",
    "    input_img = np.transpose(input_img, [2, 0, 1])\n",
    "    # subtract the mean to normalize the image, their Github code says to do this\n",
    "    mean_bgr = [103.334, 107.8797, 107.4072]\n",
    "    for i in range(0, 3):\n",
    "        input_img[i, :, :] = input_img[i, :, :] - mean_bgr[i]\n",
    "    \n",
    "    im = Image.fromarray(im.astype(np.uint8))\n",
    "    im = data_transforms['transform'](im)\n",
    "    return im[None].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ims = os.listdir('../famine_data/data/ims_malawi_2016/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28113"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I split up computation because sometimes the script would fail\n",
    "# doing it in pieces helps\n",
    "a = np.zeros((5000,4096))\n",
    "b = np.zeros((5000,4096))\n",
    "c = np.zeros((5000,4096))\n",
    "d = np.zeros((5000,4096))\n",
    "e = np.zeros((5000,4096))\n",
    "f = np.zeros((3113,4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5000):\n",
    "    im = filename_to_im_tensor('../../Predicting-Famine/famine_data/ims_malawi_2016/{}'.format(ims[i]))\n",
    "    a[i,:] = np.squeeze(model(im)['conv7'].numpy())\n",
    "    if i % 500 == 0:\n",
    "        print i\n",
    "        \n",
    "np.save('forward_feats_5000.npy', a)\n",
    "        \n",
    "for i in range(5000,10000):\n",
    "    im = filename_to_im_tensor('../../Predicting-Famine/famine_data/ims_malawi_2016/{}'.format(ims[i]))\n",
    "    b[i-5000,:] = np.squeeze(model(im)['conv7'].numpy())\n",
    "    if i % 500 == 0:\n",
    "        print i\n",
    "\n",
    "np.save('forward_feats_5000_10000.npy', b)\n",
    "        \n",
    "for i in range(10000,15000):\n",
    "    im = filename_to_im_tensor('../../Predicting-Famine/famine_data/ims_malawi_2016/{}'.format(ims[i]))\n",
    "    c[i-10000,:] = np.squeeze(model(im)['conv7'].numpy())\n",
    "    if i % 500 == 0:\n",
    "        print i\n",
    "        \n",
    "np.save('forward_feats_10000_15000.npy', c)\n",
    "        \n",
    "for i in range(15000,20000):\n",
    "    im = filename_to_im_tensor('../../Predicting-Famine/famine_data/ims_malawi_2016/{}'.format(ims[i]))\n",
    "    d[i-15000,:] = np.squeeze(model(im)['conv7'].numpy())\n",
    "    if i % 500 == 0:\n",
    "        print i\n",
    "        \n",
    "np.save('forward_feats_15000_20000.npy', d)\n",
    "        \n",
    "for i in range(20000,25000):\n",
    "    im = filename_to_im_tensor('../../Predicting-Famine/famine_data/ims_malawi_2016/{}'.format(ims[i]))\n",
    "    e[i-20000,:] = np.squeeze(model(im)['conv7'].numpy())\n",
    "    if i % 500 == 0:\n",
    "        print i\n",
    "        \n",
    "np.save('forward_feats_20000_25000.npy', e)\n",
    "        \n",
    "for i in range(25000,len(ims)):\n",
    "    im = filename_to_im_tensor('../../Predicting-Famine/famine_data/ims_malawi_2016/{}'.format(ims[i]))\n",
    "    f[i-25000,:] = np.squeeze(model(im)['conv7'].numpy())\n",
    "    if i % 500 == 0:\n",
    "        print i\n",
    "        \n",
    "np.save('forward_feats_final.npy', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
