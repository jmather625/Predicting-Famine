{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CNN weights that were provided by the paper authors. These create a .npy file called `forward_feats.npy`, which we use here.\n",
    "\n",
    "Run everything in `setup_existing_model` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.load('setup_existing_model/forward_feats.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = os.listdir('famine_data/ims_malawi_2016/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-14.416666000000001_34.466666.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-15.741666_34.491665999999995.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-14.075_35.174999.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-13.791666000000001_33.733332.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-13.541666000000001_32.816666.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              images\n",
       "0  -14.416666000000001_34.466666.png\n",
       "1  -15.741666_34.491665999999995.png\n",
       "2              -14.075_35.174999.png\n",
       "3  -13.791666000000001_33.733332.png\n",
       "4  -13.541666000000001_32.816666.png"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_im_raw = pd.DataFrame.from_dict({'images': ims}); df_im_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_to_cons = pd.read_csv('famine_data/all_ims_guide.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>im_lat</th>\n",
       "      <th>im_lon</th>\n",
       "      <th>clust_lat</th>\n",
       "      <th>clust_lon</th>\n",
       "      <th>nightlight</th>\n",
       "      <th>consumption</th>\n",
       "      <th>images</th>\n",
       "      <th>clust_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.05_35.174999_0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.125000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.125_35.174999_0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.05_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.058333</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.058332999999998_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.100000</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.1_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      im_lat     im_lon  clust_lat  clust_lon  nightlight  consumption  \\\n",
       "0 -17.050000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "1 -17.125000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "2 -17.050000  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "3 -17.058333  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "4 -17.100000  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "\n",
       "                                images  clust_num  \n",
       "0               -17.05_35.174999_0.png          0  \n",
       "1              -17.125_35.174999_0.png          0  \n",
       "2               -17.05_35.183332_0.png          0  \n",
       "3  -17.058332999999998_35.183332_0.png          0  \n",
       "4                -17.1_35.183332_0.png          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_to_cons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the original image name\n",
    "def parse_im(x):\n",
    "    x = x.split('_')\n",
    "    return x[0] + '_' + x[1] + '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_to_cons['im_original'] = im_to_cons['images'].apply(parse_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>im_lat</th>\n",
       "      <th>im_lon</th>\n",
       "      <th>clust_lat</th>\n",
       "      <th>clust_lon</th>\n",
       "      <th>nightlight</th>\n",
       "      <th>consumption</th>\n",
       "      <th>images</th>\n",
       "      <th>clust_num</th>\n",
       "      <th>im_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.05_35.174999_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.05_35.174999.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.125000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.125_35.174999_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.125_35.174999.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.05_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.05_35.183332.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.058333</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.058332999999998_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.058332999999998_35.183332.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.100000</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.1_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.1_35.183332.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      im_lat     im_lon  clust_lat  clust_lon  nightlight  consumption  \\\n",
       "0 -17.050000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "1 -17.125000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "2 -17.050000  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "3 -17.058333  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "4 -17.100000  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "\n",
       "                                images  clust_num  \\\n",
       "0               -17.05_35.174999_0.png          0   \n",
       "1              -17.125_35.174999_0.png          0   \n",
       "2               -17.05_35.183332_0.png          0   \n",
       "3  -17.058332999999998_35.183332_0.png          0   \n",
       "4                -17.1_35.183332_0.png          0   \n",
       "\n",
       "                         im_original  \n",
       "0               -17.05_35.174999.png  \n",
       "1              -17.125_35.174999.png  \n",
       "2               -17.05_35.183332.png  \n",
       "3  -17.058332999999998_35.183332.png  \n",
       "4                -17.1_35.183332.png  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_to_cons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54404, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_to_cons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigns an index to each image in the raw download\n",
    "# this index corresponds to the row in \"forward_feats.npy\"\n",
    "df_im_raw['feat_index'] = np.arange(len(df_im_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_to_cons = pd.merge(left=im_to_cons, right=df_im_raw, left_on='im_original', right_on='images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54404, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_to_cons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>im_lat</th>\n",
       "      <th>im_lon</th>\n",
       "      <th>clust_lat</th>\n",
       "      <th>clust_lon</th>\n",
       "      <th>nightlight</th>\n",
       "      <th>consumption</th>\n",
       "      <th>images_x</th>\n",
       "      <th>clust_num</th>\n",
       "      <th>im_original</th>\n",
       "      <th>images_y</th>\n",
       "      <th>feat_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.05_35.174999_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.05_35.174999.png</td>\n",
       "      <td>-17.05_35.174999.png</td>\n",
       "      <td>25523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.125000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.125_35.174999_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.125_35.174999.png</td>\n",
       "      <td>-17.125_35.174999.png</td>\n",
       "      <td>11065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.05_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.05_35.183332.png</td>\n",
       "      <td>-17.05_35.183332.png</td>\n",
       "      <td>4638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.058333</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.058332999999998_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.058332999999998_35.183332.png</td>\n",
       "      <td>-17.058332999999998_35.183332.png</td>\n",
       "      <td>21516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.100000</td>\n",
       "      <td>35.183332</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>-17.1_35.183332_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.1_35.183332.png</td>\n",
       "      <td>-17.1_35.183332.png</td>\n",
       "      <td>25154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      im_lat     im_lon  clust_lat  clust_lon  nightlight  consumption  \\\n",
       "0 -17.050000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "1 -17.125000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "2 -17.050000  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "3 -17.058333  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "4 -17.100000  35.183332  -17.09515  35.217213         0.0     2.039307   \n",
       "\n",
       "                              images_x  clust_num  \\\n",
       "0               -17.05_35.174999_0.png          0   \n",
       "1              -17.125_35.174999_0.png          0   \n",
       "2               -17.05_35.183332_0.png          0   \n",
       "3  -17.058332999999998_35.183332_0.png          0   \n",
       "4                -17.1_35.183332_0.png          0   \n",
       "\n",
       "                         im_original                           images_y  \\\n",
       "0               -17.05_35.174999.png               -17.05_35.174999.png   \n",
       "1              -17.125_35.174999.png              -17.125_35.174999.png   \n",
       "2               -17.05_35.183332.png               -17.05_35.183332.png   \n",
       "3  -17.058332999999998_35.183332.png  -17.058332999999998_35.183332.png   \n",
       "4                -17.1_35.183332.png                -17.1_35.183332.png   \n",
       "\n",
       "   feat_index  \n",
       "0       25523  \n",
       "1       11065  \n",
       "2        4638  \n",
       "3       21516  \n",
       "4       25154  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now have a dataframe that tells us what index to look at in forward_feats.npy for each image\n",
    "im_to_cons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = im_to_cons.groupby(['clust_lat', 'clust_lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusts = len(group); num_clusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((num_clusts, 4096))\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this goes through each cluster group and finds all images that are in the cluster\n",
    "# it aggregates the features for those images across the cluster\n",
    "for i, g in enumerate(group):\n",
    "    lat, long = g[0]\n",
    "    im_sub = im_to_cons[(im_to_cons['clust_lat'] == lat) & (im_to_cons['clust_lon'] == long)].reset_index(drop=True)\n",
    "    agg_feats = np.zeros((len(im_sub), 4096))\n",
    "    for j, d in im_sub.iterrows():\n",
    "        agg_feats[j,:] = feats[d.feat_index]\n",
    "    agg_feats = agg_feats.mean(axis=0) # averages the features across all images in the cluster\n",
    "    \n",
    "    x[i,:] = agg_feats\n",
    "    y.append(g[1]['consumption'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "y_log = np.log(y) # try predicting consumption and log consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780, 4096)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bunch of code from the Jean et al Github that is modified to work with Python3 and our data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.linear_model as linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import EllipseCollection\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def predict_consumption(\n",
    "    X, y, dimension=None, k=5, k_inner=5, points=10,\n",
    "        alpha_low=1, alpha_high=5, margin=0.25):\n",
    "    \"\"\"\n",
    "    Plots predicted consumption\n",
    "    \"\"\"\n",
    "    X = reduce_dimension(X, dimension)\n",
    "    y_hat, r2 = run_cv(X, y, k, k_inner, points, alpha_low, alpha_high)\n",
    "    return X, y, y_hat, r2\n",
    "\n",
    "\n",
    "def plot_predictions(country, y, y_hat, r2, margin):\n",
    "    \"\"\"\n",
    "    Plots consumption predictions vs. true values.\n",
    "    \"\"\"\n",
    "    slope, intercept, ymin, ymax, xmin, xmax = compute_plot_params(\n",
    "        y, y_hat, margin)\n",
    "    sns.set_style('white')\n",
    "    plt.figure()\n",
    "    plt.axis('equal')\n",
    "    plt.scatter(y, y_hat, edgecolor='k', color='lightblue', s=20, marker='o')\n",
    "    x_trend = np.array([xmin, xmax]) * 1.5\n",
    "    y_trend = slope * x_trend + intercept\n",
    "    plt.plot(x_trend, y_trend, 'b-', linewidth=2,\n",
    "             color=sns.xkcd_rgb['french blue'])\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.xlabel('Log consumption expenditures', fontsize=14)\n",
    "    plt.ylabel('Model predictions', fontsize=14)\n",
    "    plt.title(country.capitalize() + ': $r^2 = {0:.2f}$'.format(r2))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_plot_params(y, y_hat, margin):\n",
    "    \"\"\"\n",
    "    Computes parameters for plotting consumption predictions vs. true values.\n",
    "    \"\"\"\n",
    "    slope, intercept, _, _, _ = stats.linregress(y, y_hat)\n",
    "    ymin = min(y_hat) - margin\n",
    "    ymax = max(y_hat) + margin\n",
    "    xmin = min(y) - margin\n",
    "    xmax = max(y) + margin\n",
    "    return slope, intercept, ymin, ymax, xmin, xmax\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    country_path, survey, percentiles, dimension, k, k_inner, trials,\n",
    "        poverty_line, multiples):\n",
    "    \"\"\"\n",
    "    Evaluates and plots comparison of transfer learning and nightlight models.\n",
    "    \"\"\"\n",
    "    r2s, r2s_nl = evaluate_percentiles(\n",
    "        country_path, survey, percentiles, dimension, k, k_inner, trials)\n",
    "    if survey == 'lsms':\n",
    "        X, X_nl, y = load_and_reduce_country_by_percentile(\n",
    "            country_path, survey, 1.0, dimension)\n",
    "        fractions = compute_fractions(poverty_line, multiples, y)\n",
    "        plot_percentiles_lsms(percentiles, multiples, r2s, r2s_nl, fractions)\n",
    "    elif survey == 'dhs':\n",
    "        plot_percentiles_dhs(percentiles, r2s, r2s_nl)\n",
    "\n",
    "\n",
    "def load_and_reduce_country_by_percentile(\n",
    "        country_path, survey, percentile, dimension):\n",
    "    \"\"\"\n",
    "    Loads data for one country up to a certain percentile.\n",
    "    \"\"\"\n",
    "    if survey == 'lsms':\n",
    "        X, X_nl, y = load_country_lsms(country_path)\n",
    "    elif survey == 'dhs':\n",
    "        X, X_nl, y = load_country_dhs(country_path)\n",
    "    X, X_nl, y = threshold_by_percentile(X, X_nl, y, percentile)\n",
    "    X = reduce_dimension(X, dimension)\n",
    "    return X, X_nl, y\n",
    "\n",
    "\n",
    "def threshold_by_percentile(X, X_nl, y, percentile):\n",
    "    \"\"\"\n",
    "    Threshold data by output percentile.\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(y, q=100*percentile)\n",
    "    X = X[y <= threshold]\n",
    "    X_nl = X_nl[y <= threshold]\n",
    "    y = y[y <= threshold]\n",
    "    return X, X_nl, y\n",
    "\n",
    "\n",
    "def evaluate_percentiles(\n",
    "        country_path, survey, percentiles, dimension, k, k_inner, trials):\n",
    "    \"\"\"\n",
    "    Evaluate transfer learning and nightlight models for each percentile.\n",
    "    \"\"\"\n",
    "    r2s = np.zeros((len(percentiles), trials))\n",
    "    r2s_nl = np.zeros((len(percentiles), trials))\n",
    "    for idx, percentile in enumerate(percentiles):\n",
    "        for trial in xrange(trials):\n",
    "            X, X_nl, y = load_and_reduce_country_by_percentile(\n",
    "                country_path, survey, percentile, dimension)\n",
    "            _, r2 = run_cv(\n",
    "                X, y, k, k_inner, points=10, alpha_low=0, alpha_high=3,\n",
    "                randomize=False)\n",
    "            r2_nl = run_cv_ols(X_nl, y, k)\n",
    "            r2s[idx, trial] = r2\n",
    "            r2s_nl[idx, trial] = r2_nl\n",
    "    r2s = r2s.mean(axis=1)\n",
    "    r2s_nl = r2s_nl.mean(axis=1)\n",
    "    return r2s, r2s_nl\n",
    "\n",
    "\n",
    "def run_cv_ols(X, y, k):\n",
    "    \"\"\"\n",
    "    Runs OLS in cross-validation to compute r-squared.\n",
    "    \"\"\"\n",
    "    r2s = np.zeros((k,))\n",
    "    kf = cross_validation.KFold(n=y.size, n_folds=k, shuffle=True)\n",
    "    fold = 0\n",
    "    for train_idx, test_idx in kf:\n",
    "        r2s, fold = evaluate_fold_ols(X, y, train_idx, test_idx, r2s, fold)\n",
    "    return r2s.mean()\n",
    "\n",
    "\n",
    "def evaluate_fold_ols(X, y, train_idx, test_idx, r2s, fold):\n",
    "    \"\"\"\n",
    "    Evaluates one fold of outer CV using OLS.\n",
    "    \"\"\"\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    y_test_hat = train_and_predict_ols(X_train, y_train, X_test)\n",
    "    r2 = stats.pearsonr(y_test, y_test_hat)[0] ** 2\n",
    "    if np.isnan(r2):\n",
    "        r2 = 0\n",
    "    r2s[fold] = r2\n",
    "    return r2s, fold + 1\n",
    "\n",
    "\n",
    "def train_and_predict_ols(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Trains OLS model and predicts test set.\n",
    "    \"\"\"\n",
    "    ols = linear_model.LinearRegression()\n",
    "    ols.fit(X_train, y_train)\n",
    "    y_hat = ols.predict(X_test)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def compute_fractions(poverty_line, multiples, y):\n",
    "    \"\"\"\n",
    "    Computes the fraction of clusters below each multiple of the poverty line.\n",
    "    \"\"\"\n",
    "    fractions = np.zeros((len(multiples),))\n",
    "    for idx, multiple in enumerate(multiples):\n",
    "        fractions[idx] = (\n",
    "            np.exp(y) <= poverty_line * multiple).sum() / float(y.size)\n",
    "    return fractions\n",
    "\n",
    "\n",
    "def plot_percentiles_lsms(percentiles, multiples, r2s, r2s_nl, fractions):\n",
    "    \"\"\"\n",
    "    Plots transfer learning model vs. nightlights model at each percentile.\n",
    "    \"\"\"\n",
    "    sns.set_style('white')\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    lines = []\n",
    "    percentiles = [100 * x for x in percentiles]\n",
    "    for idx, multiple in enumerate(multiples):\n",
    "        lines.append(\n",
    "            plt.axvline(\n",
    "                100 * fractions[idx], color='r', linestyle='dashed',\n",
    "                linewidth=3.0 / (idx + 1),\n",
    "                label=str(multiple) + 'x poverty line'))\n",
    "    line_legend = plt.legend(\n",
    "        handles=lines, title='Poverty line multiples:', loc='upper right',\n",
    "        bbox_to_anchor=(0.5, 1), fontsize=10)\n",
    "    plt.gca().add_artist(line_legend)\n",
    "    curve1, = plt.plot(percentiles, r2s, label='Transfer learning')\n",
    "    curve2, = plt.plot(percentiles, r2s_nl, label='Nightlights')\n",
    "    plt.legend(\n",
    "        handles=[curve1, curve2], loc='upper right',\n",
    "        bbox_to_anchor=(0.5, 0.65))\n",
    "    plt.xlabel('Poorest percent of clusters used', fontsize=14)\n",
    "    plt.ylabel('$r^2$', fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_percentiles_dhs(percentiles, r2s, r2s_nl):\n",
    "    \"\"\"\n",
    "    Plots transfer learning model vs. nightlights model at each\n",
    "    \"\"\"\n",
    "    sns.set_style('white')\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    percentiles = [100 * x for x in percentiles]\n",
    "    plt.plot(percentiles, r2s)\n",
    "    plt.plot(percentiles, r2s_nl)\n",
    "    plt.legend(['Transfer learning', 'Nightlights'], loc='upper center')\n",
    "    plt.xlabel('Poorest percent of clusters used', fontsize=14)\n",
    "    plt.ylabel('$r^2$', fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_randomization_test(\n",
    "    country_names, country_paths, survey, dimension, k, k_inner, points,\n",
    "        alpha_low, alpha_high, trials):\n",
    "    \"\"\"\n",
    "    Runs randomization test for a set of countries.\n",
    "    \"\"\"\n",
    "    data = load_data(country_paths, survey, dimension)\n",
    "    true_r2s = compute_true_r2s(\n",
    "        data, k, k_inner, points, alpha_low, alpha_high)\n",
    "    shuffled_r2s = compute_shuffled_r2s(\n",
    "        data, k, k_inner, points, alpha_low, alpha_high, trials)\n",
    "    plot_shuffled_distributions(country_names, survey, shuffled_r2s, true_r2s)\n",
    "\n",
    "\n",
    "def compute_true_r2s(data, k, k_inner, points, alpha_low, alpha_high):\n",
    "    \"\"\"\n",
    "    Uses data to compute true model r2s.\n",
    "    \"\"\"\n",
    "    true_r2s = []\n",
    "    for (X, y) in data:\n",
    "        _, r2 = run_cv(X, y, k, k_inner, points, alpha_low, alpha_high)\n",
    "        true_r2s.append(r2)\n",
    "    return true_r2s\n",
    "\n",
    "\n",
    "def compute_shuffled_r2s(\n",
    "        data, k, k_inner, points, alpha_low, alpha_high, trials):\n",
    "    \"\"\"\n",
    "    Uses data to compute shuffled model r2s.\n",
    "    \"\"\"\n",
    "    shuffled_r2s = np.zeros((len(data), trials))\n",
    "    for data_idx, (X, y) in enumerate(data):\n",
    "        for trial in range(trials):\n",
    "            _, shuffled_r2s[data_idx, trial] = run_cv(\n",
    "                X, y, k, k_inner, points, alpha_low, alpha_high,\n",
    "                randomize=True)\n",
    "    return shuffled_r2s\n",
    "\n",
    "\n",
    "def plot_shuffled_distributions(country_names, survey, shuffled_r2s, true_r2s):\n",
    "    \"\"\"\n",
    "    Plots shuffled r2 distributions vs. true model r2s.\n",
    "    \"\"\"\n",
    "    colors = sns.color_palette('husl', len(country_names))\n",
    "    sns.set_style('white')\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    frame = plt.gca()\n",
    "    for i in xrange(len(country_names)):\n",
    "        plt.axvline(\n",
    "            true_r2s[i], color=colors[i], linestyle='dashed', linewidth=2,\n",
    "            label=country_names[i].capitalize() + ': $r^2={0:.2f}$'.format(\n",
    "                true_r2s[i]))\n",
    "    plt.legend(\n",
    "        title='True {} Models:'.format(survey.upper()), loc='lower right',\n",
    "        bbox_to_anchor=(0.6, 0.45), fontsize=12)\n",
    "    for i in xrange(len(country_names)):\n",
    "        sns.kdeplot(shuffled_r2s[i, :], shade=True, color=colors[i])\n",
    "    plt.xlim((0, max(true_r2s) + 0.05))\n",
    "    plt.xlabel('$r^2$', fontsize=18)\n",
    "    plt.ylabel('Randomized $r^2$ distribution', fontsize=14)\n",
    "    frame.yaxis.set_ticklabels([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_models(\n",
    "    country_names, country_paths, survey, dimension, k, trials, points,\n",
    "        alpha_low, alpha_high, cmap='Greens'):\n",
    "    \"\"\"\n",
    "    Evaluates in- and out-of-country performance for models trained on set of\n",
    "    countries given.\n",
    "    \"\"\"\n",
    "    n = len(country_names)\n",
    "    performance_matrix = np.zeros((trials, n, n))\n",
    "    data = load_data(country_paths, survey, dimension=None)\n",
    "    for trial in xrange(trials):\n",
    "        for in_idx in xrange(n):\n",
    "            performance_matrix[trial, :, in_idx] = compute_column(\n",
    "                country_names[in_idx], data, in_idx, dimension, k, points,\n",
    "                alpha_low, alpha_high)\n",
    "    performance_matrix = performance_matrix.mean(axis=0)\n",
    "    plot_model_performance(performance_matrix, country_names, cmap)\n",
    "    return np.around(performance_matrix, decimals=2)\n",
    "\n",
    "\n",
    "def compute_column(\n",
    "    country_name, data, in_idx, dimension, k, points, alpha_low,\n",
    "        alpha_high):\n",
    "    \"\"\"\n",
    "    Evaluates in- and out-of-country performance for model trained on one\n",
    "    country.\n",
    "    \"\"\"\n",
    "    in_data = data[in_idx]\n",
    "    out_data = list(data)\n",
    "    out_data.pop(in_idx)\n",
    "    if country_name == 'pooled':\n",
    "        r2s_in, r2s_out = evaluate_pooled_r2s(\n",
    "            out_data, dimension, k, points, alpha_low, alpha_high)\n",
    "    else:\n",
    "        r2s_in, r2s_out = evaluate_model_r2s(\n",
    "            in_data, out_data, dimension, k, points, alpha_low, alpha_high)\n",
    "    max_idx = np.argmax(r2s_in)\n",
    "    r2s = list(r2s_out[:, max_idx])\n",
    "    r2s.insert(in_idx, r2s_in[max_idx])\n",
    "    r2s.reverse()\n",
    "    r2s = np.array(r2s)\n",
    "    return r2s\n",
    "\n",
    "\n",
    "def evaluate_pooled_r2s(all_data, dimension, k, points, alpha_low, alpha_high):\n",
    "    \"\"\"\n",
    "    Evaluates in- and out-of-country r2s for the pooled model.\n",
    "    \"\"\"\n",
    "    X_all_full, y_all = [list(data) for data in zip(*all_data)]\n",
    "    alphas = np.logspace(alpha_low, alpha_high, points)\n",
    "    r2s_in = np.zeros((k, points))\n",
    "    r2s_out = np.zeros((len(all_data), k, points))\n",
    "    kf = []\n",
    "    for i in xrange(len(X_all_full)):\n",
    "        kf.append(list(cross_validation.KFold(\n",
    "            n=all_data[i][1].size, n_folds=k, shuffle=True)))\n",
    "    for fold in xrange(k):\n",
    "        X_train, X_test, y_train, y_test = split_pooled_data(\n",
    "            kf, fold, X_all_full, y_all)\n",
    "        X_train, X_test, X_all = reduce_and_scale_features(\n",
    "            X_train, X_test, X_all_full, dimension)\n",
    "        r2s_in, r2s_out = evaluate_alphas(\n",
    "            X_train, X_test, X_all, y_train, y_test, y_all, r2s_in, r2s_out,\n",
    "            fold, alphas)\n",
    "    return r2s_in.mean(axis=0), r2s_out.mean(axis=1)\n",
    "\n",
    "\n",
    "def split_pooled_data(kf, fold, X_all_full, y_all):\n",
    "    \"\"\"\n",
    "    Splits pooled training and test data for each fold.\n",
    "    \"\"\"\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i in xrange(len(kf)):\n",
    "        train_ind = kf[i][fold][0]\n",
    "        test_ind = kf[i][fold][1]\n",
    "        X_train.append(X_all_full[i][train_ind])\n",
    "        X_test.append(X_all_full[i][test_ind])\n",
    "        y_train.append(y_all[i][train_ind])\n",
    "        y_test.append(y_all[i][test_ind])\n",
    "    X_train = np.vstack(X_train)\n",
    "    X_test = np.vstack(X_test)\n",
    "    y_train = np.hstack(y_train)\n",
    "    y_test = np.hstack(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def evaluate_model_r2s(\n",
    "        in_data, out_data, dimension, k, points, alpha_low, alpha_high):\n",
    "    \"\"\"\n",
    "    Evaluates in- and out-of-country r2s.\n",
    "    \"\"\"\n",
    "    X, y = in_data\n",
    "    X_out_full, y_out = [list(data) for data in zip(*out_data)]\n",
    "    alphas = np.logspace(alpha_low, alpha_high, points)\n",
    "    r2s_in = np.zeros((k, points))\n",
    "    r2s_out = np.zeros((len(out_data), k, points))\n",
    "    kf = cross_validation.KFold(n=in_data[1].size, n_folds=k, shuffle=True)\n",
    "    fold = 0\n",
    "    for train_ind, test_ind in kf:\n",
    "        X_train, X_test = X[train_ind], X[test_ind]\n",
    "        y_train, y_test = y[train_ind], y[test_ind]\n",
    "        X_train, X_test, X_out = reduce_and_scale_features(\n",
    "            X_train, X_test, X_out_full, dimension)\n",
    "        r2s_in, r2s_out = evaluate_alphas(\n",
    "            X_train, X_test, X_out, y_train, y_test, y_out, r2s_in, r2s_out,\n",
    "            fold, alphas)\n",
    "        fold += 1\n",
    "    return r2s_in.mean(axis=0), r2s_out.mean(axis=1)\n",
    "\n",
    "\n",
    "def evaluate_alphas(\n",
    "    X_train, X_test, X_out, y_train, y_test, y_out, r2s_in, r2s_out, fold,\n",
    "        alphas):\n",
    "    \"\"\"\n",
    "    Computes r2 for different regularization constants.\n",
    "    \"\"\"\n",
    "    for idx, alpha in enumerate(alphas):\n",
    "        ridge = linear_model.Ridge(alpha=alpha)\n",
    "        ridge.fit(X_train.astype(np.float), y_train.astype(np.float))\n",
    "        r2s_in[fold, idx] = compute_r2(ridge, X_test, y_test)\n",
    "        for i in xrange(len(X_out)):\n",
    "            r2s_out[i, fold, idx] = compute_r2(ridge, X_out[i], y_out[i])\n",
    "    return r2s_in, r2s_out\n",
    "\n",
    "\n",
    "def reduce_and_scale_features(X_train, X_test, X_out_full, dimension):\n",
    "    \"\"\"\n",
    "    Reduces dimension and scales in- and out-of-country features.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=dimension)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "    X_train = scaler.fit_transform(pca.fit_transform(X_train))\n",
    "    X_test = scaler.transform(pca.transform(X_test))\n",
    "    X_out = []\n",
    "    for X in X_out_full:\n",
    "        X_out.append(scaler.transform(pca.transform(X)))\n",
    "    return X_train, X_test, X_out\n",
    "\n",
    "\n",
    "def compute_r2(model, X, y):\n",
    "    \"\"\"\n",
    "    Computes model r2.\n",
    "    \"\"\"\n",
    "    y_hat = model.predict(X)\n",
    "    r2 = stats.pearsonr(y, y_hat)[0] ** 2\n",
    "    return r2\n",
    "\n",
    "\n",
    "def plot_circles(data, ax, **kwargs):\n",
    "    \"\"\"\n",
    "    Plots circles for r2 values.\n",
    "    \"\"\"\n",
    "    M = np.array(data)\n",
    "    xy = np.indices(M.shape)[::-1].reshape(2, -1).T\n",
    "    w = np.abs(M).ravel()\n",
    "    h = np.abs(M).ravel()\n",
    "    a = 0\n",
    "    circles = EllipseCollection(\n",
    "        widths=w, heights=h, angles=a, units='x', offsets=xy,\n",
    "        transOffset=ax.transData, array=M.ravel(), **kwargs)\n",
    "    ax.add_collection(circles)\n",
    "    ax.set_xticks(np.arange(M.shape[1]))\n",
    "    ax.set_xticklabels(data.columns)\n",
    "    ax.set_yticks(np.arange(M.shape[0]))\n",
    "    ax.set_yticklabels(data.index, rotation=90)\n",
    "    return circles\n",
    "\n",
    "\n",
    "def plot_model_performance(data, country_names, cmap='Greens'):\n",
    "    \"\"\"\n",
    "    Makes plot for in- and out-of-country model performance.\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(np.flipud(data))\n",
    "    data.columns = [country.capitalize() for country in country_names]\n",
    "    data.index = [country.capitalize() for country in country_names]\n",
    "    sns.set_style('white')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    circles = plot_circles(data, ax=ax, cmap=cmap)\n",
    "    cb = fig.colorbar(circles)\n",
    "    cb.set_label('$r^2$', fontsize=16)\n",
    "    ax.margins(0.1)\n",
    "    plt.xlabel('Country trained on', fontsize=16)\n",
    "    plt.ylabel('Country evaluated on', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_data(country_paths, survey, dimension):\n",
    "    \"\"\"\n",
    "    Loads data for all surveys.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for country_path in country_paths:\n",
    "        X, y = load_and_reduce_country(country_path, survey, dimension)\n",
    "        data.append((X, y))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_country_lsms(country_path):\n",
    "    \"\"\"\n",
    "    Loads data for one LSMS country.\n",
    "    \"\"\"\n",
    "    X = np.load(country_path + 'cluster_conv_features.npy')\n",
    "    X_nl = np.load(country_path + 'cluster_nightlights.npy').reshape(-1, 1)\n",
    "    y = np.load(country_path + 'cluster_consumptions.npy')\n",
    "    hhs = np.load(country_path + 'cluster_households.npy')\n",
    "    images = np.load(country_path + 'cluster_image_counts.npy')\n",
    "    # Filter out single households and <10 images\n",
    "    mask = np.logical_and((hhs >= 2), (images >= 10))\n",
    "    X = X[mask]\n",
    "    X_nl = X_nl[mask]\n",
    "    y = y[mask]\n",
    "    # Filter out 0 consumption clusters\n",
    "    X = X[y > 0]\n",
    "    X_nl = X_nl[y > 0]\n",
    "    y = y[y > 0]\n",
    "    y = np.log(y)\n",
    "    return X, X_nl, y\n",
    "\n",
    "\n",
    "def load_country_dhs(country_path):\n",
    "    \"\"\"\n",
    "    Loads data for one DHS country.\n",
    "    \"\"\"\n",
    "    X = np.load(country_path + 'cluster_conv_features.npy')\n",
    "    X_nl = np.load(country_path + 'cluster_nightlights.npy').reshape(-1, 1)\n",
    "    y = np.load(country_path + 'cluster_assets.npy')\n",
    "    hhs = np.load(country_path + 'cluster_households.npy')\n",
    "    images = np.load(country_path + 'cluster_image_counts.npy')\n",
    "    # Filter out single households and <10 images\n",
    "    mask = np.logical_and((hhs >= 2), (images >= 10))\n",
    "    X = X[mask]\n",
    "    X_nl = X_nl[mask]\n",
    "    y = y[mask]\n",
    "    return X, X_nl, y\n",
    "\n",
    "\n",
    "def reduce_dimension(X, dimension):\n",
    "    \"\"\"\n",
    "    Uses PCA to reduce dimensionality of features.\n",
    "    \"\"\"\n",
    "    if dimension is not None:\n",
    "        pca = PCA(n_components=dimension)\n",
    "        X = pca.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def load_and_reduce_country(country_path, survey, dimension):\n",
    "    \"\"\"\n",
    "    Loads data for one country and reduces the dimensionality of features.\n",
    "    \"\"\"\n",
    "    if survey == 'lsms':\n",
    "        X, _, y = load_country_lsms(country_path)\n",
    "    elif survey == 'dhs':\n",
    "        X, _, y = load_country_dhs(country_path)\n",
    "    X = reduce_dimension(X, dimension)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_cv(X, y, k, k_inner, points, alpha_low, alpha_high, randomize=False):\n",
    "    \"\"\"\n",
    "    Runs nested cross-validation to make predictions and compute r-squared.\n",
    "    \"\"\"\n",
    "    alphas = np.logspace(alpha_low, alpha_high, points)\n",
    "    r2s = np.zeros((k,))\n",
    "    y_hat = np.zeros_like(y)\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    fold = 0\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        r2s, y_hat, fold = evaluate_fold(\n",
    "            X, y, train_idx, test_idx, k_inner, alphas, r2s, y_hat, fold,\n",
    "            randomize)\n",
    "    return y_hat, r2s.mean()\n",
    "\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales features using StandardScaler.\n",
    "    \"\"\"\n",
    "    X_scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "    X_train = X_scaler.fit_transform(X_train)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def train_and_predict_ridge(alpha, X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Trains ridge model and predicts test set.\n",
    "    \"\"\"\n",
    "    ridge = linear_model.Ridge(alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_hat = ridge.predict(X_test)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def predict_inner_test_fold(X, y, y_hat, train_idx, test_idx, alpha):\n",
    "    \"\"\"\n",
    "    Predicts inner test fold.\n",
    "    \"\"\"\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    y_hat[test_idx] = train_and_predict_ridge(alpha, X_train, y_train, X_test)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def find_best_alpha(X, y, k_inner, alphas):\n",
    "    \"\"\"\n",
    "    Finds the best alpha in an inner CV loop.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k_inner, shuffle=True)\n",
    "    best_alpha = 0\n",
    "    best_r2 = 0\n",
    "    for idx, alpha in enumerate(alphas):\n",
    "        y_hat = np.zeros_like(y)\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            y_hat = predict_inner_test_fold(\n",
    "                X, y, y_hat, train_idx, test_idx, alpha)\n",
    "        r2 = stats.pearsonr(y, y_hat)[0] ** 2\n",
    "        if r2 > best_r2:\n",
    "            best_alpha = alpha\n",
    "            best_r2 = r2\n",
    "    print('best alpha', best_alpha)\n",
    "    return best_alpha\n",
    "\n",
    "\n",
    "def evaluate_fold(\n",
    "    X, y, train_idx, test_idx, k_inner, alphas, r2s, y_hat, fold,\n",
    "        randomize):\n",
    "    \"\"\"\n",
    "    Evaluates one fold of outer CV.\n",
    "    \"\"\"\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    if randomize:\n",
    "        random.shuffle(y_train)\n",
    "    best_alpha = find_best_alpha(X_train, y_train, k_inner, alphas)\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    y_test_hat = train_and_predict_ridge(best_alpha, X_train, y_train, X_test)\n",
    "    r2 = stats.pearsonr(y_test, y_test_hat)[0] ** 2\n",
    "    r2s[fold] = r2\n",
    "    y_hat[test_idx] = y_test_hat\n",
    "    return r2s, y_hat, fold + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x_hat = ss.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha 4641.588833612777\n",
      "best alpha 599.4842503189409\n",
      "best alpha 1668.100537200059\n",
      "best alpha 12915.496650148827\n",
      "best alpha 599.4842503189409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1364787608685853"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, r2 = predict_consumption(x_hat, y_log)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I try doing it myself, without using their function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.7*(len(x_hat)))\n",
    "inds = np.arange(len(x_hat))\n",
    "train_ind = np.random.choice(inds, n_train, replace=False)\n",
    "valid_ind = np.delete(inds, train_ind)\n",
    "\n",
    "train_x = x_hat[train_ind]\n",
    "valid_x = x_hat[valid_ind]\n",
    "\n",
    "train_y = y_log[train_ind]\n",
    "valid_y = y_log[valid_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1574371149034828, 1.1162793609556527)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.mean(), valid_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5730434718080026"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = linear_model.Ridge()\n",
    "ridge.fit(train_x, train_y)\n",
    "ridge.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2428218088979825"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what happened is that ridge regression is heavily affected by alpha (regularization) value\n",
    "# their code finds a good alpha, which we do not do here\n",
    "# if we use theirs, our ridge score goes up\n",
    "ridge.score(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3322913182434448"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = linear_model.Ridge(alpha=2000)\n",
    "ridge.fit(train_x, train_y)\n",
    "ridge.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14102874147953304"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(valid_x, valid_y) # as we expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
