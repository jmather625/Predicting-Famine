{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Inspired by https://arxiv.org/pdf/1609.02907.pdf\n", "\n", "Install everything in the Readme"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import torch\n", "import numpy as np\n", "import scipy.sparse as sp"]}, {"cell_type": "markdown", "metadata": {}, "source": ["GCN spec:\n", "\n", "Needs: <br>\n", "-adjacency list of size nxn <br>\n", "-output of size nx1 <br>\n", "-classification or regression\n", "\n", "Can also provide: <br>\n", "-initial features of size nxf <br>\n", "-hidden layer size (hl) <br>\n", "-train/test split on nodes\n", "\n", "\n", "TODO: <br>\n", "-sticking it on a GPU, which really just means adding .cuda() and a function parameter <br>\n", "-parameterize `predict` function, so you can predict all nodes or just a subset (train/test); easy to do"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Hand-written GCN class"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["class GCN:\n", "    def __init__(self, adj_list, out_labels, method, nl=3, init_feats=None, train_test=0.5):\n", "        \"\"\"\n", "        Creates important model variables and weights\n", "        method: 'cat' or 'cont'\n", "        \"\"\"\n", "        self.adj_list = torch.from_numpy(adj_list).double()\n", "        sqrt = True # turn to False to use 'simple' adj_list normalization\n", "        D = GCN.create_D(self.adj_list, sqrt=sqrt)\n", "        if sqrt:\n", "            self.norm_adj_list = D @ self.adj_list @ D # https://tkipf.github.io/graph-convolutional-networks/#fn3\n", "        else:\n", "            self.norm_adj_list = D @ self.adj_list\n", "        self.out_labels = torch.from_numpy(out_labels).view(1, -1)\n", "        self.method = method\n", "        if self.method == 'cat':\n", "            self.nc = len(torch.unique(self.out_labels))\n", "        # if no features given, use identity matrix\n", "        if init_feats is None:\n", "            init_feats = np.eye(len(adj_list))\n", "        self.init_feats = torch.from_numpy(init_feats).double()\n", "        self.nl = nl\n", "        # can give train indices directly or give a train/test split\n", "        if type(train_test) == np.ndarray:\n", "            self.train_indices = train_test\n", "        else:\n", "            self.train_indices = np.random.choice(np.arange(len(adj_list)), int(train_test*len(adj_list)), replace=False)\n", "        \n", "        self.weight_list = []\n", "        self.bias_list = []\n", "        if self.method == 'cat':\n", "            sizes = [max(40 - 5*i, 2*self.nc) for i in range(self.nl)]\n", "        else:\n", "            sizes = [max(40 - 5*i, 20) for i in range(self.nl)]\n", "        sizes[-1] = self.nc if self.method == 'cat' else 1 # 1 for continuous output\n", "        for i in range(self.nl):\n", "            if i == 0:\n", "                w = torch.randn(init_feats.shape[1], sizes[0], dtype=torch.double, requires_grad=True)\n", "            else:\n", "                w = torch.randn(sizes[i-1], sizes[i], dtype=torch.double, requires_grad=True)\n", "            b = torch.randn(sizes[i], dtype=torch.double, requires_grad=True)\n", "            \n", "            self.weight_list.append(w)\n", "            self.bias_list.append(b)\n", "            \n", "        self.lr = 1e-2\n", "        self.epoch_stats = []\n", "        \n", "        \n", "    def train(self, epochs=100):\n", "        for i in range(epochs):\n", "            self.train_epoch()\n", "            \n", "            \n", "    def train_epoch(self):\n", "        \"\"\"\n", "        Propogates the model through the GCN using the formula:\n", "            L_next = A_hat @ L_cur @ w_cur\n", "            where A_hat is the normalized adj matrix\n", "        Then, backprops the model and updates the weights\n", "        Stores, the loss in self.epoch_stats\n", "        \"\"\"\n", "        l_prev = None\n", "        for i, (w,b) in enumerate(zip(self.weight_list, self.bias_list)):\n", "            if i == 0:\n", "                l_prev = self.gcn_layer(self.norm_adj_list, self.init_feats, w, b, 'relu')\n", "            elif i != len(self.weight_list)-1:\n", "                l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, b, 'relu')\n", "            else:\n", "                if self.method == 'cat':\n", "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, b, 'softmax') # predictions!\n", "                else:\n", "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, b, None) # no activation for final layer\n", "                    \n", "        loss = self.compute_loss(l_prev, self.out_labels)\n", "        self.epoch_stats.append(loss)\n", "        \n", "        # backprop\n", "        loss.backward() # torch is amazing.. :)\n", "        with torch.no_grad():\n", "            for i in range(len(self.weight_list)):\n", "                self.weight_list[i] -= self.lr * self.weight_list[i].grad # updates weight\n", "                self.weight_list[i].grad.zero_() # resets to 0\n", "                \n", "                \n", "    def predict(self):\n", "        if self.method == 'cat':\n", "            return self.predict_cat()\n", "        else:\n", "            return self.predict_cont()\n", "        \n", "        \n", "    def predict_cont(self):\n", "        with torch.no_grad():\n", "            l_prev = None\n", "            for i, (w, b) in enumerate(zip(self.weight_list, self.bias_list)):\n", "                if i == 0:\n", "                    l_prev = self.gcn_layer(self.norm_adj_list, self.init_feats, w, b, 'relu')\n", "                elif i != len(self.weight_list)-1:\n", "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, b, 'relu')\n", "                else:\n", "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, b, None) # predictions!\n", "            return l_prev\n", "        \n", "        \n", "    def predict_cat(self):\n", "        with torch.no_grad():\n", "            l_prev = None\n", "            for i, (w, b) in enumerate(zip(self.weight_list, self.bias_list)):\n", "                if i == 0:\n", "                    l_prev = self.gcn_layer(self.norm_adj_list, self.init_feats, w, b, 'relu')\n", "                elif i != len(self.weight_list)-1:\n", "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, b, 'relu')\n", "                else:\n", "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, b, 'softmax') # predictions!\n", "            return torch.argmax(l_prev, dim=1)\n", "    \n", "    \n", "    def compute_loss(self, preds, actual):\n", "        if self.method == 'cat':\n", "            return self.compute_cat_loss(preds, actual)\n", "        else:\n", "            return self.compute_cont_loss(preds, actual)\n", "        \n", "        \n", "    def compute_cont_loss(self, preds, actual):\n", "        \"\"\"\n", "        Uses MSE loss\n", "        \"\"\"\n", "        masked_actual = actual.clone().detach().view(-1,1)\n", "        masked_actual[~self.train_indices,:] = 0 # if not a train index, set to 0\n", "        preds *= (masked_actual != 0).double()\n", "        # by doing this, we are calculating the loss for the vertices specified as everything else is 0\n", "        # this will also make sure the loss is only fed (via sum) from rows with train nodes\n", "        # hence, weights are only updated with regards to this loss\n", "        # and the GCN effectively still doesn't know about the vertices it never got the actual loss for !\n", "        return ((preds - masked_actual)**2).sum()\n", "        \n", "    \n", "    def compute_cat_loss(self, preds, actual):\n", "        \"\"\"\n", "        Uses categorical cross-entropy loss\n", "        \"\"\"\n", "        # this assigns each class a unique index. This is helpful becase the classes could be 2 and 8\n", "        # for all I know. This will assign 2 to 0 and 8 to 1\n", "        classes = torch.unique(actual).numpy()\n", "        mapping = {}\n", "        for i in range(len(classes)):\n", "            mapping[classes[i]] = i\n", "        # this makes a one-hot matrix based on the number of classes\n", "        baseline = torch.eye(self.nc)\n", "        # this is the final one-hot, where each row corresponds to the actual output in one-hot form\n", "        one_hot = np.zeros(preds.shape, dtype=np.double)\n", "        # this grabs the vertices that we can actually use to train and assigns the row in \n", "        # one-hot to the actual one-hot value (so if we are at vertex 2 and class=3, we turn \n", "        # row 2 into 0 1 0 0 ... 0)\n", "        for i, v in enumerate(actual.numpy().reshape(-1)[self.train_indices]):\n", "            index = self.train_indices[i]\n", "            one_hot[index,:] = baseline[mapping[v]].numpy().tolist()\n", "        one_hot = torch.from_numpy(one_hot)\n", "        preds = -torch.log(preds)\n", "        # by doing this, we are calculating the loss for the vertices specified as everything else is 0\n", "        # this will also make sure the loss is only fed (via sum) from rows with non-zero one-hot\n", "        # aka the vertices we set in the previous for loop\n", "        # hence, weights are only updated with regards to this loss\n", "        # and the GCN effectively still doesn't know about the vertices it never got the actual loss for !\n", "        return (preds * one_hot).sum()\n", "    \n", "        \n", "    def create_D(adj_list, sqrt=True):\n", "        # creates the matrix for making A_hat\n", "        # sqrt is optional, can simply invert the diagonal as well, but the paper recommends this approach\n", "        D = torch.eye(len(adj_list), dtype=torch.double)\n", "        diag_sums = adj_list.sum(dim=0)\n", "        for i in range(len(adj_list)):\n", "            if sqrt:\n", "                D[i,i] = 1/torch.sqrt(diag_sums[i])\n", "            else:\n", "                D[i,i] = 1/diag_sums[i]\n", "        return D\n", "    \n", "    \n", "    def gcn_layer(self, ad_list, inp, weights, bias, activation):\n", "        \"\"\"\n", "        Propogates the model through the GCN using the formula:\n", "            L_next = A_hat @ L_cur @ w_cur + bias\n", "            where A_hat is the normalized adj matrix\n", "        \"\"\"\n", "        out = ad_list @ inp @ weights + bias\n", "        if activation == 'relu':\n", "            return torch.relu(out)\n", "        elif activation == 'softmax':\n", "            return torch.softmax(out, dim=0)\n", "        elif activation is None:\n", "            return out\n", "        else:\n", "            raise ValueError()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Karate Club Example\n", "https://en.wikipedia.org/wiki/Zachary%27s_karate_club"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["import networkx"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["G = networkx.karate_club_graph()"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["adj_dict = dict(G.adjacency())"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["adj_m = np.eye(len(adj_dict.keys()))\n", "for k in adj_dict:\n", "    for n in adj_dict[k]:\n", "        adj_m[k,n] = 1"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[1., 1., 1., ..., 1., 0., 0.],\n", "       [1., 1., 1., ..., 0., 0., 0.],\n", "       [1., 1., 1., ..., 0., 1., 0.],\n", "       ...,\n", "       [1., 0., 0., ..., 1., 1., 1.],\n", "       [0., 0., 1., ..., 1., 1., 1.],\n", "       [0., 0., 0., ..., 1., 1., 1.]])"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["adj_m # this has self loops, so A is a neighbor of A; this is good"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["out_labels = []\n", "for d in G.nodes:\n", "    if G.nodes[d]['club'] == 'Mr. Hi':\n", "        out_labels.append(0)\n", "    else:\n", "        out_labels.append(1)\n", "out_labels = np.array(out_labels)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n", "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["out_labels # 0 = Mr. Hi, 1 = the other guy"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# known = [3, 5, 24, 28]\n", "# known = np.array(known) # can also pass in this into \"train_test\" to show which nodes are known"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# giving the model about 0.3 of the data makes it predict nearly every node correctly\n", "gcn = GCN(adj_m, out_labels, method='cat', nl=3, init_feats=None, train_test=0.2)"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": ["tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n", "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1])"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["preds_init = gcn.predict(); preds_init # nonsense initial predictions"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n", "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": ["out_labels"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.5"]}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": ["(preds_init.numpy() == out_labels).mean() # very bad clearly"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["gcn.train(epochs=40)"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.7058823529411765"]}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": ["preds_init = gcn.predict()\n", "(preds_init.numpy() == out_labels).mean() # does this well with just 0.2 of the nodes"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"data": {"text/plain": ["[tensor(38.4060, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(37.2317, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(36.0834, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(34.9536, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(33.8325, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(32.7415, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(31.6803, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(30.6531, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(29.6572, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(28.6935, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(27.7612, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(26.8718, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(26.0283, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(25.2252, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(24.4649, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(23.7506, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(23.0905, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(22.4895, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(21.9528, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(21.4705, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(21.0430, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(20.6690, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(20.3454, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(20.0675, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(19.8295, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(19.6274, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(19.4530, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(19.2997, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(19.1644, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(19.0441, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.9347, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.8328, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.7391, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.6522, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.5716, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.4963, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.4255, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.3586, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.2950, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(18.2345, dtype=torch.float64, grad_fn=<SumBackward0>)]"]}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": ["gcn.epoch_stats"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Cora"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["def encode_onehot(labels):\n", "    classes = set(labels)\n", "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n", "                    enumerate(classes)}\n", "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n", "                             dtype=np.int32)\n", "    return labels_onehot\n", "\n", "def normalize(mx):\n", "    \"\"\"Row-normalize sparse matrix\"\"\"\n", "    rowsum = np.array(mx.sum(1))\n", "    r_inv = np.power(rowsum, -1).flatten()\n", "    r_inv[np.isinf(r_inv)] = 0.\n", "    r_mat_inv = sp.diags(r_inv)\n", "    mx = r_mat_inv.dot(mx)\n", "    return mx"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# from https://github.com/tkipf/gcn\n", "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n", "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n", "    print('Loading {} dataset...'.format(dataset))\n", "\n", "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n", "                                        dtype=np.dtype(str))\n", "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n", "    labels = encode_onehot(idx_features_labels[:, -1])\n", "\n", "    # build graph\n", "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n", "    idx_map = {j: i for i, j in enumerate(idx)}\n", "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n", "                                    dtype=np.int32)\n", "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n", "                     dtype=np.int32).reshape(edges_unordered.shape)\n", "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n", "                        shape=(labels.shape[0], labels.shape[0]),\n", "                        dtype=np.float32)\n", "\n", "    # build symmetric adjacency matrix\n", "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n", "\n", "    features = normalize(features)\n", "    adj = normalize(adj + sp.eye(adj.shape[0]))\n", "\n", "    idx_train = range(140)\n", "    idx_val = range(200, 500)\n", "    idx_test = range(500, 1500)\n", "    \n", "    labels = np.where(labels)[1]\n", "\n", "    return adj, features, labels, idx_train, idx_val, idx_test"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Loading cora dataset...\n"]}], "source": ["adj, features, labels, idx_train, idx_val, idx_test = load_data()"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"data": {"text/plain": ["(2708, 2708)"]}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": ["adj = adj.todense(); adj.shape"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"data": {"text/plain": ["matrix([[0., 0., 0., ..., 0., 0., 0.],\n", "        [0., 0., 0., ..., 0., 0., 0.],\n", "        [0., 0., 0., ..., 0., 0., 0.],\n", "        ...,\n", "        [0., 0., 0., ..., 0., 0., 0.],\n", "        [0., 0., 0., ..., 0., 0., 0.],\n", "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"]}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": ["features = features.todense(); features"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([5, 0, 1, ..., 2, 6, 5])"]}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": ["labels"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"data": {"text/plain": ["range(0, 140)"]}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": ["idx_train"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"data": {"text/plain": ["range(200, 500)"]}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": ["idx_val"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"data": {"text/plain": ["range(500, 1500)"]}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": ["idx_test"]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n", "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n", "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n", "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n", "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n", "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n", "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n", "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n", "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n", "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n", "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139])"]}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": ["known = np.array(list(idx_train)); known"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["gcn = GCN(adj, labels, method='cat', nl=3, init_feats=features, train_test=known)"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"data": {"text/plain": ["tensor([2, 2, 2,  ..., 4, 4, 0])"]}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}], "source": ["init_preds = gcn.predict(); init_preds"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.16838995568685378"]}, "execution_count": 30, "metadata": {}, "output_type": "execute_result"}], "source": ["(init_preds.numpy() == labels).mean()"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["CPU times: user 4min 5s, sys: 4.92 s, total: 4min 10s\n", "Wall time: 23.9 s\n"]}], "source": ["%time gcn.train(150)"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"data": {"text/plain": ["tensor([5, 1, 1,  ..., 2, 6, 0])"]}, "execution_count": 32, "metadata": {}, "output_type": "execute_result"}], "source": ["preds = gcn.predict(); preds"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.4250369276218611"]}, "execution_count": 33, "metadata": {}, "output_type": "execute_result"}], "source": ["(preds.numpy() == labels).mean()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Hand made dataset\n", "This corresponds to the drawing in `proof_of_concept.jpg`"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["adj_m = [\n", "        [1,1,1,0,0,0],\n", "        [1,1,1,0,0,1],\n", "        [1,1,1,1,1,0],\n", "        [0,0,1,1,1,0],\n", "        [0,0,1,1,1,1],\n", "        [0,1,0,0,1,1]\n", "                    ]"]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["init_features = [\n", "                [1, 5, 11, 0, 0, 0],\n", "                [5, 1, 6, 0, 0, 13],\n", "                [11, 6, 1, 7, 6, 0],\n", "                [0, 0, 7, 1, 3, 0],\n", "                [0, 0, 6, 3, 1, 6],\n", "                [0, 13, 0, 0, 6, 1]\n", "                                    ]"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["labels = [0, 0, 1, 1, 1, 1] # NOTE, only index 0, 2, and 3 are actually known\n", "known = [0, 2, 3]"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["adj_m = np.array(adj_m)\n", "init_feats = np.array(init_features, dtype=np.double)\n", "labels = np.array(labels)\n", "known = np.array(known)"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["np.place(init_feats, init_feats==0, 1000)"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[   1.,    5.,   11., 1000., 1000., 1000.],\n", "       [   5.,    1.,    6., 1000., 1000.,   13.],\n", "       [  11.,    6.,    1.,    7.,    6., 1000.],\n", "       [1000., 1000.,    7.,    1.,    3., 1000.],\n", "       [1000., 1000.,    6.,    3.,    1.,    6.],\n", "       [1000.,   13., 1000., 1000.,    6.,    1.]])"]}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": ["init_feats"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0.00033146, 0.00049383, 0.00096993, 0.00033212, 0.00049603,\n", "       0.00033113])"]}, "execution_count": 40, "metadata": {}, "output_type": "execute_result"}], "source": ["# normalizes each row\n", "norm_init = 1/init_feats.sum(axis=0); norm_init"]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[3.31455088e-04, 1.65727544e-03, 3.64600597e-03, 3.31455088e-01,\n", "        3.31455088e-01, 3.31455088e-01],\n", "       [2.46913580e-03, 4.93827160e-04, 2.96296296e-03, 4.93827160e-01,\n", "        4.93827160e-01, 6.41975309e-03],\n", "       [1.06692532e-02, 5.81959263e-03, 9.69932105e-04, 6.78952473e-03,\n", "        5.81959263e-03, 9.69932105e-01],\n", "       [3.32115576e-01, 3.32115576e-01, 2.32480903e-03, 3.32115576e-04,\n", "        9.96346729e-04, 3.32115576e-01],\n", "       [4.96031746e-01, 4.96031746e-01, 2.97619048e-03, 1.48809524e-03,\n", "        4.96031746e-04, 2.97619048e-03],\n", "       [3.31125828e-01, 4.30463576e-03, 3.31125828e-01, 3.31125828e-01,\n", "        1.98675497e-03, 3.31125828e-04]])"]}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}], "source": ["norm_init_feats = init_feats * norm_init.reshape(-1,1); norm_init_feats"]}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [], "source": ["gcn = GCN(adj_m, labels, method='cat', nl=3, init_feats=norm_init_feats, train_test=known)"]}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"data": {"text/plain": ["tensor([0, 1, 1, 0, 0, 0])"]}, "execution_count": 43, "metadata": {}, "output_type": "execute_result"}], "source": ["preds_init = gcn.predict(); preds_init # randomly initial predictions"]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": ["gcn.train(25)"]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [{"data": {"text/plain": ["tensor([0, 1, 1, 0, 0, 0])"]}, "execution_count": 45, "metadata": {}, "output_type": "execute_result"}], "source": ["preds = gcn.predict(); preds # pretty much what intuition suggests !!"]}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [{"data": {"text/plain": ["[tensor(7.2859, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(7.2443, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(7.2034, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(7.1629, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(7.1226, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(7.0826, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(7.0429, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(7.0034, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.9643, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.9247, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.8830, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.8415, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.8005, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.7597, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.7193, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.6793, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.6396, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.6002, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.5612, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.5225, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.4842, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.4462, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.4085, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.3712, dtype=torch.float64, grad_fn=<SumBackward0>),\n", " tensor(6.3342, dtype=torch.float64, grad_fn=<SumBackward0>)]"]}, "execution_count": 46, "metadata": {}, "output_type": "execute_result"}], "source": ["gcn.epoch_stats # loss over epochs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Continuous Dataset"]}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [], "source": ["adj_m = [\n", "        [1,1,1,0,0,0],\n", "        [1,1,1,0,0,1],\n", "        [1,1,1,1,1,0],\n", "        [0,0,1,1,1,0],\n", "        [0,0,1,1,1,1],\n", "        [0,1,0,0,1,1]\n", "                    ]"]}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [], "source": ["init_features = [\n", "                [1, 5, 11, 0, 0, 0],\n", "                [5, 1, 6, 0, 0, 13],\n", "                [11, 6, 1, 7, 6, 0],\n", "                [0, 0, 7, 1, 3, 0],\n", "                [0, 0, 6, 3, 1, 6],\n", "                [0, 13, 0, 0, 6, 1]\n", "                                    ]"]}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [], "source": ["labels = [-0.4, -0.2, 0, 0.1, 0.2, 0.3] # NOTE, only index 0, 2, and 3 are actually known\n", "known = [0, 2, 3]"]}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": ["adj_m = np.array(adj_m)\n", "init_feats = np.array(init_features, dtype=np.double)\n", "labels = np.array(labels)\n", "known = np.array(known)"]}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [], "source": ["np.place(init_feats, init_feats==0, 1000)"]}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[   1.,    5.,   11., 1000., 1000., 1000.],\n", "       [   5.,    1.,    6., 1000., 1000.,   13.],\n", "       [  11.,    6.,    1.,    7.,    6., 1000.],\n", "       [1000., 1000.,    7.,    1.,    3., 1000.],\n", "       [1000., 1000.,    6.,    3.,    1.,    6.],\n", "       [1000.,   13., 1000., 1000.,    6.,    1.]])"]}, "execution_count": 52, "metadata": {}, "output_type": "execute_result"}], "source": ["init_feats"]}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0.00033146, 0.00049383, 0.00096993, 0.00033212, 0.00049603,\n", "       0.00033113])"]}, "execution_count": 53, "metadata": {}, "output_type": "execute_result"}], "source": ["# normalizes each row\n", "norm_init = 1/init_feats.sum(axis=0); norm_init"]}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[3.31455088e-04, 1.65727544e-03, 3.64600597e-03, 3.31455088e-01,\n", "        3.31455088e-01, 3.31455088e-01],\n", "       [2.46913580e-03, 4.93827160e-04, 2.96296296e-03, 4.93827160e-01,\n", "        4.93827160e-01, 6.41975309e-03],\n", "       [1.06692532e-02, 5.81959263e-03, 9.69932105e-04, 6.78952473e-03,\n", "        5.81959263e-03, 9.69932105e-01],\n", "       [3.32115576e-01, 3.32115576e-01, 2.32480903e-03, 3.32115576e-04,\n", "        9.96346729e-04, 3.32115576e-01],\n", "       [4.96031746e-01, 4.96031746e-01, 2.97619048e-03, 1.48809524e-03,\n", "        4.96031746e-04, 2.97619048e-03],\n", "       [3.31125828e-01, 4.30463576e-03, 3.31125828e-01, 3.31125828e-01,\n", "        1.98675497e-03, 3.31125828e-04]])"]}, "execution_count": 54, "metadata": {}, "output_type": "execute_result"}], "source": ["norm_init_feats = init_feats * norm_init.reshape(-1,1); norm_init_feats"]}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [], "source": ["gcn = GCN(adj_m, labels, method='cont', nl=3, init_feats=norm_init_feats, train_test=known)"]}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [{"data": {"text/plain": ["tensor([[3.9744],\n", "        [4.3756],\n", "        [5.7932],\n", "        [5.0821],\n", "        [5.3348],\n", "        [3.5987]], dtype=torch.float64)"]}, "execution_count": 56, "metadata": {}, "output_type": "execute_result"}], "source": ["preds_init = gcn.predict(); preds_init # randomly initial predictions"]}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [], "source": ["gcn.train(100)"]}, {"cell_type": "code", "execution_count": 58, "metadata": {}, "outputs": [{"data": {"text/plain": ["tensor([[-0.3749],\n", "        [-0.3240],\n", "        [ 0.0775],\n", "        [ 0.3513],\n", "        [ 0.3049],\n", "        [ 0.0840]], dtype=torch.float64)"]}, "execution_count": 58, "metadata": {}, "output_type": "execute_result"}], "source": ["preds_init = gcn.predict(); preds_init # not perfect but it captures the trend, only so much u can do w small dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "SPIN-famine", "language": "python", "name": "spin-famine"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}