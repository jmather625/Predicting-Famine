{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by https://arxiv.org/pdf/1609.02907.pdf\n",
    "\n",
    "Install everything in the Readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN spec:\n",
    "\n",
    "Needs: <br>\n",
    "-adjacency list of size nxn <br>\n",
    "-output of size nx1 <br>\n",
    "-classification or regression\n",
    "\n",
    "Can also provide: <br>\n",
    "-initial features of size nxf <br>\n",
    "-hidden layer size (hl) <br>\n",
    "-train/test split on nodes\n",
    "\n",
    "\n",
    "TODO: <br>\n",
    "-sticking it on a GPU, which really just means adding .cuda() and a function parameter <br>\n",
    "-parameterize `predict` function, so you can predict all nodes or just a subset (train/test); easy to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-written GCN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN:\n",
    "    def __init__(self, adj_list, out_labels, method, nl=3, init_feats=None, train_test=0.5):\n",
    "        \"\"\"\n",
    "        Creates important model variables and weights\n",
    "        method: 'cat' or 'cont'\n",
    "        \"\"\"\n",
    "        # assertions\n",
    "        assert len(adj_list) == len(adj_list[0])\n",
    "        assert len(out_labels) == len(adj_list)\n",
    "        assert method in ['cat', 'cont']\n",
    "        assert type(nl) == int and nl >= 2 # input and output at minimum\n",
    "        assert init_feats is None or len(init_feats) == len(adj_list)\n",
    "        assert type(train_test) == np.ndarray or (type(train_test) == float and train_test <= 1 and train_test > 0)\n",
    "        \n",
    "        self.adj_list = torch.from_numpy(adj_list).double()\n",
    "        sqrt = True # turn to False to use 'simple' adj_list normalization\n",
    "        D = GCN.create_D(self.adj_list, sqrt=sqrt)\n",
    "        if sqrt:\n",
    "            self.norm_adj_list = D @ self.adj_list @ D # https://tkipf.github.io/graph-convolutional-networks/#fn3\n",
    "        else:\n",
    "            self.norm_adj_list = D @ self.adj_list\n",
    "        self.out_labels = torch.from_numpy(out_labels).view(1, -1)\n",
    "        self.method = method\n",
    "        if self.method == 'cat':\n",
    "            self.nc = len(torch.unique(self.out_labels))\n",
    "        # if no features given, use identity matrix\n",
    "        if init_feats is None:\n",
    "            init_feats = np.eye(len(adj_list))\n",
    "        self.init_feats = torch.from_numpy(init_feats)\n",
    "        self.nl = nl\n",
    "        # can give train indices directly or give a train/test split\n",
    "        if type(train_test) == np.ndarray:\n",
    "            self.train_indices = train_test\n",
    "        else:\n",
    "            self.train_indices = np.random.choice(np.arange(len(adj_list)), int(train_test*len(adj_list)), replace=False)\n",
    "        \n",
    "        self.weight_list = []\n",
    "        if self.method == 'cat':\n",
    "            sizes = [max(40 - 5*i, 2*self.nc) for i in range(self.nl)]\n",
    "        else:\n",
    "            sizes = [max(40 - 5*i, 20) for i in range(self.nl)]\n",
    "        sizes[-1] = self.nc if self.method == 'cat' else 1 # 1 for continuous output\n",
    "        for i in range(self.nl):\n",
    "            if i == 0:\n",
    "                w = torch.randn(init_feats.shape[1], sizes[0], dtype=torch.double, requires_grad=True)\n",
    "            else:\n",
    "                w = torch.randn(sizes[i-1], sizes[i], dtype=torch.double, requires_grad=True)\n",
    "            self.weight_list.append(w)\n",
    "            \n",
    "        self.lr = 1e-4\n",
    "        self.epoch_stats = []\n",
    "        \n",
    "        \n",
    "    def train(self, epochs=100):\n",
    "        for i in range(epochs):\n",
    "            self.train_epoch()\n",
    "            \n",
    "            \n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Propogates the model through the GCN using the formula:\n",
    "            L_next = A_hat @ L_cur @ w_cur\n",
    "            where A_hat is the normalized adj matrix\n",
    "        Then, backprops the model and updates the weights\n",
    "        Stores, the loss in self.epoch_stats\n",
    "        \"\"\"\n",
    "        l_prev = None\n",
    "        for i, w in enumerate(self.weight_list):\n",
    "            if i == 0:\n",
    "                l_prev = self.gcn_layer(self.norm_adj_list, self.init_feats, w, 'relu')\n",
    "            elif i != len(self.weight_list)-1:\n",
    "                l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, 'relu')\n",
    "            else:\n",
    "                if self.method == 'cat':\n",
    "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, 'softmax') # predictions!\n",
    "                else:\n",
    "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, None) # no activation for final layer\n",
    "                    \n",
    "        loss = self.compute_loss(l_prev, self.out_labels)\n",
    "        self.epoch_stats.append(loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward() # torch is amazing.. :)\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.weight_list)):\n",
    "                self.weight_list[i] -= self.lr * self.weight_list[i].grad # updates weight\n",
    "                self.weight_list[i].grad.zero_() # resets to 0\n",
    "                \n",
    "                \n",
    "    def predict(self):\n",
    "        if self.method == 'cat':\n",
    "            return self.predict_cat()\n",
    "        else:\n",
    "            return self.predict_cont()\n",
    "        \n",
    "        \n",
    "    def predict_cont(self):\n",
    "        with torch.no_grad():\n",
    "            l_prev = None\n",
    "            for i, w in enumerate(self.weight_list):\n",
    "                if i == 0:\n",
    "                    l_prev = self.gcn_layer(self.norm_adj_list, self.init_feats, w, 'relu')\n",
    "                elif i != len(self.weight_list)-1:\n",
    "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, 'relu')\n",
    "                else:\n",
    "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, None) # predictions!\n",
    "            return l_prev\n",
    "        \n",
    "        \n",
    "    def predict_cat(self):\n",
    "        with torch.no_grad():\n",
    "            l_prev = None\n",
    "            for i, w in enumerate(self.weight_list):\n",
    "                if i == 0:\n",
    "                    l_prev = self.gcn_layer(self.norm_adj_list, self.init_feats, w, 'relu')\n",
    "                elif i != len(self.weight_list)-1:\n",
    "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, 'relu')\n",
    "                else:\n",
    "                    l_prev = self.gcn_layer(self.norm_adj_list, l_prev, w, 'softmax') # predictions!\n",
    "            return torch.argmax(l_prev, axis=1)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, preds, actual):\n",
    "        if self.method == 'cat':\n",
    "            return self.compute_cat_loss(preds, actual)\n",
    "        else:\n",
    "            return self.compute_cont_loss(preds, actual)\n",
    "        \n",
    "        \n",
    "    def compute_cont_loss(self, preds, actual):\n",
    "        \"\"\"\n",
    "        Uses MSE loss\n",
    "        \"\"\"\n",
    "        masked_actual = actual.clone().detach().view(-1,1)\n",
    "        masked_actual[~self.train_indices,:] = 0 # if not a train index, set to 0\n",
    "        preds *= (masked_actual != 0).double()\n",
    "        # by doing this, we are calculating the loss for the vertices specified as everything else is 0\n",
    "        # this will also make sure the loss is only fed (via sum) from rows with train nodes\n",
    "        # hence, weights are only updated with regards to this loss\n",
    "        # and the GCN effectively still doesn't know about the vertices it never got the actual loss for !\n",
    "        return ((preds - masked_actual)**2).sum()\n",
    "        \n",
    "    \n",
    "    def compute_cat_loss(self, preds, actual):\n",
    "        \"\"\"\n",
    "        Uses categorical cross-entropy loss\n",
    "        \"\"\"\n",
    "        # this assigns each class a unique index. This is helpful becase the classes could be 2 and 8\n",
    "        # for all I know. This will assign 2 to 0 and 8 to 1\n",
    "        classes = torch.unique(actual).numpy()\n",
    "        mapping = {}\n",
    "        for i in range(len(classes)):\n",
    "            mapping[classes[i]] = i\n",
    "        # this makes a one-hot matrix based on the number of classes\n",
    "        baseline = torch.eye(self.nc)\n",
    "        # this is the final one-hot, where each row corresponds to the actual output in one-hot form\n",
    "        one_hot = np.zeros(preds.shape, dtype=np.double)\n",
    "        # this grabs the vertices that we can actually use to train and assigns the row in \n",
    "        # one-hot to the actual one-hot value (so if we are at vertex 2 and class=3, we turn \n",
    "        # row 2 into 0 1 0 0 ... 0)\n",
    "        for i, v in enumerate(actual.numpy().reshape(-1)[self.train_indices]):\n",
    "            index = self.train_indices[i]\n",
    "            one_hot[index,:] = baseline[mapping[v]].numpy().tolist()\n",
    "        one_hot = torch.from_numpy(one_hot)\n",
    "        preds = -torch.log(preds)\n",
    "        # by doing this, we are calculating the loss for the vertices specified as everything else is 0\n",
    "        # this will also make sure the loss is only fed (via sum) from rows with non-zero one-hot\n",
    "        # aka the vertices we set in the previous for loop\n",
    "        # hence, weights are only updated with regards to this loss\n",
    "        # and the GCN effectively still doesn't know about the vertices it never got the actual loss for !\n",
    "        return (preds * one_hot).sum()\n",
    "    \n",
    "        \n",
    "    def create_D(adj_list, sqrt=True):\n",
    "        # creates the matrix for making A_hat\n",
    "        # sqrt is optional, can simply invert the diagonal as well, but the paper recommends this approach\n",
    "        D = torch.eye(len(adj_list), dtype=torch.double)\n",
    "        diag_sums = adj_list.sum(dim=0)\n",
    "        for i in range(len(adj_list)):\n",
    "            if sqrt:\n",
    "                D[i,i] = 1/torch.sqrt(diag_sums[i])\n",
    "            else:\n",
    "                D[i,i] = 1/diag_sums[i]\n",
    "        return D\n",
    "    \n",
    "    \n",
    "    def gcn_layer(self, ad_list, inp, weights, activation):\n",
    "        \"\"\"\n",
    "        Propogates the model through the GCN using the formula:\n",
    "            L_next = A_hat @ L_cur @ w_cur\n",
    "            where A_hat is the normalized adj matrix\n",
    "        \"\"\"\n",
    "        out = ad_list @ inp @ weights\n",
    "        if activation == 'relu':\n",
    "            return torch.relu(out)\n",
    "        elif activation == 'softmax':\n",
    "            return torch.softmax(out, dim=0)\n",
    "        elif activation is None:\n",
    "            return out\n",
    "        else:\n",
    "            raise ValueError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karate Club Example\n",
    "https://en.wikipedia.org/wiki/Zachary%27s_karate_club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = networkx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict = dict(G.adjacency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_m = np.eye(len(adj_dict.keys()))\n",
    "for k in adj_dict:\n",
    "    for n in adj_dict[k]:\n",
    "        adj_m[k,n] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 1., 1., 1.],\n",
       "       [0., 0., 1., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_m # this has self loops, so A is a neighbor of A; this is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_labels = []\n",
    "for d in G.nodes:\n",
    "    if G.nodes[d]['club'] == 'Mr. Hi':\n",
    "        out_labels.append(0)\n",
    "    else:\n",
    "        out_labels.append(1)\n",
    "out_labels = np.array(out_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_labels # 0 = Mr. Hi, 1 = the other guy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# known = [3, 5, 24, 28]\n",
    "# known = np.array(known) # can also pass in this into \"train_test\" to show which nodes are known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving the model about 0.3 of the data makes it predict nearly every node correctly\n",
    "gcn = GCN(adj_m, out_labels, method='cat', nl=3, init_feats=None, train_test=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_init = gcn.predict(); preds_init # nonsense initial predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3235294117647059"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds_init.numpy() == out_labels).mean() # very bad clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn.train(epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9117647058823529"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_init = gcn.predict()\n",
    "(preds_init.numpy() == out_labels).mean() # does this well while training on only half the labeled nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(52.7011, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(35.6980, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(32.5641, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(30.8256, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(29.7124, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(28.9693, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(28.4385, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(28.0156, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(27.6558, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(27.3410, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(27.0583, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(26.8021, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(26.5675, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(26.3478, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(26.1430, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(25.9503, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(25.7699, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(25.6029, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(25.4446, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(25.2941, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(25.1511, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(25.0189, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.8894, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.7660, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.6485, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.5336, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.4250, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.3188, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.2154, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.1175, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(24.0211, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.9304, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.8421, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.7562, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.6739, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.5940, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.5186, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.4438, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.3664, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(23.2902, dtype=torch.float64, grad_fn=<SumBackward0>)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn.epoch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand made dataset\n",
    "This corresponds to the drawing in `proof_of_concept.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_m = [\n",
    "        [1,1,1,0,0,0],\n",
    "        [1,1,1,0,0,1],\n",
    "        [1,1,1,1,1,0],\n",
    "        [0,0,1,1,1,0],\n",
    "        [0,0,1,1,1,1],\n",
    "        [0,1,0,0,1,1]\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_features = [\n",
    "                [1, 5, 11, 0, 0, 0],\n",
    "                [5, 1, 6, 0, 0, 13],\n",
    "                [11, 6, 1, 7, 6, 0],\n",
    "                [0, 0, 7, 1, 3, 0],\n",
    "                [0, 0, 6, 3, 1, 6],\n",
    "                [0, 13, 0, 0, 6, 1]\n",
    "                                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 0, 1, 1, 1, 1] # NOTE, only index 0, 2, and 3 are actually known\n",
    "known = [0, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_m = np.array(adj_m)\n",
    "init_feats = np.array(init_features, dtype=np.double)\n",
    "labels = np.array(labels)\n",
    "known = np.array(known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.place(init_feats, init_feats==0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    5.,   11., 1000., 1000., 1000.],\n",
       "       [   5.,    1.,    6., 1000., 1000.,   13.],\n",
       "       [  11.,    6.,    1.,    7.,    6., 1000.],\n",
       "       [1000., 1000.,    7.,    1.,    3., 1000.],\n",
       "       [1000., 1000.,    6.,    3.,    1.,    6.],\n",
       "       [1000.,   13., 1000., 1000.,    6.,    1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00033146, 0.00049383, 0.00096993, 0.00033212, 0.00049603,\n",
       "       0.00033113])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizes each row\n",
    "norm_init = 1/init_feats.sum(axis=0); norm_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.31455088e-04, 1.65727544e-03, 3.64600597e-03, 3.31455088e-01,\n",
       "        3.31455088e-01, 3.31455088e-01],\n",
       "       [2.46913580e-03, 4.93827160e-04, 2.96296296e-03, 4.93827160e-01,\n",
       "        4.93827160e-01, 6.41975309e-03],\n",
       "       [1.06692532e-02, 5.81959263e-03, 9.69932105e-04, 6.78952473e-03,\n",
       "        5.81959263e-03, 9.69932105e-01],\n",
       "       [3.32115576e-01, 3.32115576e-01, 2.32480903e-03, 3.32115576e-04,\n",
       "        9.96346729e-04, 3.32115576e-01],\n",
       "       [4.96031746e-01, 4.96031746e-01, 2.97619048e-03, 1.48809524e-03,\n",
       "        4.96031746e-04, 2.97619048e-03],\n",
       "       [3.31125828e-01, 4.30463576e-03, 3.31125828e-01, 3.31125828e-01,\n",
       "        1.98675497e-03, 3.31125828e-04]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_init_feats = init_feats * norm_init.reshape(-1,1); norm_init_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCN(adj_m, labels, method='cat', nl=3, init_feats=norm_init_feats, train_test=known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3, 2, 1, 2])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_init = gcn.predict(); preds_init # randomly initial predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn.train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = gcn.predict(); preds # pretty much what intuition suggests !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(8.3114, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(7.9898, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(7.6924, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(7.4108, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(7.1532, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(6.9108, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(6.6719, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(6.4493, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(6.2470, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(6.0633, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(5.8927, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(5.7345, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(5.5918, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(5.4625, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(5.3448, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(5.2372, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(5.1416, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(5.0546, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(4.9735, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(4.8978, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(4.8270, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(4.7607, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(4.6989, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(4.6408, dtype=torch.float64, grad_fn=<SumBackward0>),\n",
       " tensor(4.5857, dtype=torch.float64, grad_fn=<SumBackward0>)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn.epoch_stats # loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_m = [\n",
    "        [1,1,1,0,0,0],\n",
    "        [1,1,1,0,0,1],\n",
    "        [1,1,1,1,1,0],\n",
    "        [0,0,1,1,1,0],\n",
    "        [0,0,1,1,1,1],\n",
    "        [0,1,0,0,1,1]\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_features = [\n",
    "                [1, 5, 11, 0, 0, 0],\n",
    "                [5, 1, 6, 0, 0, 13],\n",
    "                [11, 6, 1, 7, 6, 0],\n",
    "                [0, 0, 7, 1, 3, 0],\n",
    "                [0, 0, 6, 3, 1, 6],\n",
    "                [0, 13, 0, 0, 6, 1]\n",
    "                                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [-0.4, -0.2, 0, 0.1, 0.2, 0.3] # NOTE, only index 0, 2, and 3 are actually known\n",
    "known = [0, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_m = np.array(adj_m)\n",
    "init_feats = np.array(init_features, dtype=np.double)\n",
    "labels = np.array(labels)\n",
    "known = np.array(known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.place(init_feats, init_feats==0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    5.,   11., 1000., 1000., 1000.],\n",
       "       [   5.,    1.,    6., 1000., 1000.,   13.],\n",
       "       [  11.,    6.,    1.,    7.,    6., 1000.],\n",
       "       [1000., 1000.,    7.,    1.,    3., 1000.],\n",
       "       [1000., 1000.,    6.,    3.,    1.,    6.],\n",
       "       [1000.,   13., 1000., 1000.,    6.,    1.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00033146, 0.00049383, 0.00096993, 0.00033212, 0.00049603,\n",
       "       0.00033113])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizes each row\n",
    "norm_init = 1/init_feats.sum(axis=0); norm_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.31455088e-04, 1.65727544e-03, 3.64600597e-03, 3.31455088e-01,\n",
       "        3.31455088e-01, 3.31455088e-01],\n",
       "       [2.46913580e-03, 4.93827160e-04, 2.96296296e-03, 4.93827160e-01,\n",
       "        4.93827160e-01, 6.41975309e-03],\n",
       "       [1.06692532e-02, 5.81959263e-03, 9.69932105e-04, 6.78952473e-03,\n",
       "        5.81959263e-03, 9.69932105e-01],\n",
       "       [3.32115576e-01, 3.32115576e-01, 2.32480903e-03, 3.32115576e-04,\n",
       "        9.96346729e-04, 3.32115576e-01],\n",
       "       [4.96031746e-01, 4.96031746e-01, 2.97619048e-03, 1.48809524e-03,\n",
       "        4.96031746e-04, 2.97619048e-03],\n",
       "       [3.31125828e-01, 4.30463576e-03, 3.31125828e-01, 3.31125828e-01,\n",
       "        1.98675497e-03, 3.31125828e-04]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_init_feats = init_feats * norm_init.reshape(-1,1); norm_init_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCN(adj_m, labels, method='cont', nl=3, init_feats=norm_init_feats, train_test=known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8765],\n",
       "        [-0.8096],\n",
       "        [-2.4902],\n",
       "        [-2.9867],\n",
       "        [-2.6371],\n",
       "        [-0.9308]], dtype=torch.float64)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_init = gcn.predict(); preds_init # randomly initial predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5171],\n",
       "        [-0.0588],\n",
       "        [-0.4675],\n",
       "        [-0.2559],\n",
       "        [ 0.1674],\n",
       "        [ 0.6249]], dtype=torch.float64)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_init = gcn.predict(); preds_init # not perfect but it captures the trend, only so much u can do w small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPIN-famine",
   "language": "python",
   "name": "spin-famine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
